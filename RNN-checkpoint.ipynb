{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9794fcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries (pandas, numpy, re, tensorflow, and keras)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "248cdd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the train and test data from csv files using pandas and Concatenate the train and test data \n",
    "\n",
    "train_df=pd.read_csv(\"Train.csv\")\n",
    "test_df=pd.read_csv(\"Test.csv\")\n",
    "df=pd.concat([train_df,test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7aab39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the 'id' column\n",
    "df = df.drop(['id'], axis=1)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26afcda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the 'id' column\n",
    "\n",
    "train_df = train_df.drop(['id'], axis=1)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.drop(['id'], axis=1)\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a498303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates from the concatenated data, train data and test data\n",
    "\n",
    "train_df.duplicated(subset= ['article', 'highlights']).sum()\n",
    "train_df = train_df.drop_duplicates(subset= ['article', 'highlights'])\n",
    "\n",
    "test_df.duplicated(subset= ['article', 'highlights']).sum()\n",
    "test_df = test_df.drop_duplicates(subset= ['article', 'highlights'])\n",
    "\n",
    "df.duplicated(subset= ['article', 'highlights']).sum()\n",
    "df = df.drop_duplicates(subset= ['article', 'highlights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6fcbfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 2)\n"
     ]
    }
   ],
   "source": [
    "# Assign the data to variable X and y\n",
    "X = df.iloc[:,:-1].values\n",
    "y = df.iloc[:,1].values\n",
    "y = y.reshape(-1,1)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768afb5f",
   "metadata": {},
   "source": [
    "\n",
    "print(df.isna().sum())\n",
    "print(train_df.info())\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4c2323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text using the Tokenizer class \n",
    "\n",
    "vocab_size = 5000\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(df['article'])\n",
    "sequences = tokenizer.texts_to_sequences(df['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "304b9206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences to the same length\n",
    "\n",
    "max_length = max([len(s) for s in sequences])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5042dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the target variable and one hot encoded version of it\n",
    "\n",
    "target = df['highlights']\n",
    "target_classes = list(set(target))\n",
    "target_classes_num = {target_classes[i]:i for i in range(len(target_classes))}\n",
    "target_num = [target_classes_num[t] for t in target]\n",
    "target_one_hot = to_categorical(target_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65f41fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model using Embedding, LSTM, and Dense layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_length))\n",
    "model.add(LSTM(units=100, return_sequences=True))\n",
    "model.add(LSTM(units=100))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=len(target_classes), activation='softmax'))\n",
    "\n",
    "# Compile the model with 'adam' optimizer, 'categorical_crossentropy' loss, and 'accuracy' metrics\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd739f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "4/4 [==============================] - 9s 2s/step - loss: 4.8542 - accuracy: 0.0000e+00\n",
      "Epoch 2/25\n",
      "4/4 [==============================] - 11s 3s/step - loss: 4.8488 - accuracy: 0.0312\n",
      "Epoch 3/25\n",
      "4/4 [==============================] - 19s 5s/step - loss: 4.8429 - accuracy: 0.0703\n",
      "Epoch 4/25\n",
      "4/4 [==============================] - 31s 8s/step - loss: 4.8337 - accuracy: 0.0859\n",
      "Epoch 5/25\n",
      "4/4 [==============================] - 35s 9s/step - loss: 4.8139 - accuracy: 0.0625\n",
      "Epoch 6/25\n",
      "4/4 [==============================] - 37s 9s/step - loss: 4.7779 - accuracy: 0.0156\n",
      "Epoch 7/25\n",
      "4/4 [==============================] - 46s 12s/step - loss: 4.7046 - accuracy: 0.0234\n",
      "Epoch 8/25\n",
      "4/4 [==============================] - 49s 12s/step - loss: 4.6223 - accuracy: 0.0234\n",
      "Epoch 9/25\n",
      "4/4 [==============================] - 46s 12s/step - loss: 4.5256 - accuracy: 0.0234\n",
      "Epoch 10/25\n",
      "4/4 [==============================] - 55s 14s/step - loss: 4.4254 - accuracy: 0.0156\n",
      "Epoch 11/25\n",
      "4/4 [==============================] - 54s 13s/step - loss: 4.2760 - accuracy: 0.0156\n",
      "Epoch 12/25\n",
      "4/4 [==============================] - 51s 13s/step - loss: 4.1718 - accuracy: 0.0156\n",
      "Epoch 13/25\n",
      "4/4 [==============================] - 53s 14s/step - loss: 4.0838 - accuracy: 0.0312\n",
      "Epoch 14/25\n",
      "4/4 [==============================] - 53s 13s/step - loss: 3.9259 - accuracy: 0.0703\n",
      "Epoch 15/25\n",
      "4/4 [==============================] - 53s 13s/step - loss: 3.7900 - accuracy: 0.0859\n",
      "Epoch 16/25\n",
      "4/4 [==============================] - 52s 13s/step - loss: 3.6500 - accuracy: 0.1094\n",
      "Epoch 17/25\n",
      "4/4 [==============================] - 54s 14s/step - loss: 3.4842 - accuracy: 0.1328\n",
      "Epoch 18/25\n",
      "4/4 [==============================] - 55s 14s/step - loss: 3.3816 - accuracy: 0.1172\n",
      "Epoch 19/25\n",
      "4/4 [==============================] - 54s 14s/step - loss: 3.2481 - accuracy: 0.1484\n",
      "Epoch 20/25\n",
      "4/4 [==============================] - 57s 14s/step - loss: 3.1133 - accuracy: 0.2578\n",
      "Epoch 21/25\n",
      "4/4 [==============================] - 61s 15s/step - loss: 2.8915 - accuracy: 0.3047\n",
      "Epoch 22/25\n",
      "4/4 [==============================] - 56s 14s/step - loss: 2.7654 - accuracy: 0.3516\n",
      "Epoch 23/25\n",
      "4/4 [==============================] - 54s 13s/step - loss: 2.5407 - accuracy: 0.4844\n",
      "Epoch 24/25\n",
      "4/4 [==============================] - 51s 13s/step - loss: 2.4774 - accuracy: 0.4453\n",
      "Epoch 25/25\n",
      "4/4 [==============================] - 51s 13s/step - loss: 2.3891 - accuracy: 0.5312\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(padded_sequences, target_one_hot, epochs=25, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f527db53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 2.2579 - accuracy: 0.4828\n",
      "Accuracy on test dataset: 0.48275861144065857\n"
     ]
    }
   ],
   "source": [
    "# Access the training loss and accuracy history\n",
    "loss = history.history['loss']\n",
    "acc = history.history['accuracy']\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['article'])\n",
    "test_padded_sequences = pad_sequences(test_sequences, maxlen=max_length)\n",
    "test_target = test_df['highlights']\n",
    "test_target_num = [target_classes_num[t] for t in test_target]\n",
    "test_target_one_hot = to_categorical(test_target_num)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_padded_sequences, test_target_one_hot)\n",
    "print('Accuracy on test dataset:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d66ca09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
