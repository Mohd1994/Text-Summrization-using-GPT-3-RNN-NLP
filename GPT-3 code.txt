"""
Note: The code reads in two csv files 'Train.csv' and 'Test.csv' 
and concatenates them into a single dataframe 'df'. 
It then removes the 'id' column and any duplicate rows
based on the 'article' and 'highlight' columns. 
It checks for any missing values and prints out information
about the dataframe. 
Finally, it uses the OpenAI API to generate a summary of the given
'article_text' using the GPT-3 model.

"""

#Importing necessary libraries
import pandas as pd
import numpy as np
import sklearn.metrics as ms
import re
import openai
import nltk

#Downloading necessary NLTK packages
nltk.download('stopwords')
nltk.download('punkt')

#Importing necessary NLTK modules
from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()
from nltk.corpus import stopwords
ps = PorterStemmer()
from nltk import word_tokenize
import string 

# Read in data
data1 = pd.read_csv("CNN Train.csv",encoding='latin1')
data2 = pd.read_csv("CNN Test.csv",encoding='latin1')
df = pd.concat([data1, data2])

# Clean Data
df = df.drop(['id'], axis=1)
df = df.reset_index(drop=True)


data1 = data1.drop(['id'], axis=1)
data1 = data1.reset_index(drop=True)
data2 = data2.drop(['id'], axis=1)
data2 = data2.reset_index(drop=True)

#Removing duplicates from the dataframe
data1.duplicated(subset= ['article', 'highlights']).sum()
data1 = data1.drop_duplicates(subset= ['article', 'highlights'])

data2.duplicated(subset= ['article', 'highlights']).sum()
data2 = data2.drop_duplicates(subset= ['article', 'highlights'])

df.duplicated(subset= ['article', 'highlights']).sum()
df = df.drop_duplicates(subset= ['article', 'highlights'])


# Check for missing values in article column
print(df['article'].isna().sum())
df.dropna(axis=0,inplace=True)
df = df.dropna(subset=['article'])

# Reset the dataset
df = df.reset_index(drop=True)
data1 = data1.reset_index(drop=True)

#Checking for any missing values
print(df.isna().sum()) 
print(df.info()) 
print(data1.info())

#Set up the OpenAI API client
openai.api_key = "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"

# Choose the GPT-3 model to use
model_engine = "davinci"

# Load the original news article text

article_text = "By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A . State Immunization Program Manager Molly Howell says the risk is low, but officials feel it's important to alert people to the possible exposure. The diocese announced on Monday that Bishop John Folda is taking time off after being diagnosed with hepatitis A. The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located ."

# Set the desired length of the summary
summary_length = 100

# Generate the summary using the GPT-3 API

completion = openai.Completion.create(engine=model_engine,
                                      prompt=article_text,
                                      max_tokens=summary_length,
                                      n=1,
                                      stop=None,
                                      temperature=0.5)

# Extract the summary text from the API response
summary_text = completion.choices[0].text

# Print the summary text
print(summary_text)