{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea5d5771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\9941064513.UPS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65a4cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "data1 = pd.read_csv(\"CNN Train.csv\",encoding='latin1')\n",
    "data2 = pd.read_csv(\"CNN Test.csv\",encoding='latin1')\n",
    "df = pd.concat([data1, data2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaf1a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Data\n",
    "df = df.drop(['id'], axis=1)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "data1 = data1.drop(['id'], axis=1)\n",
    "data1 = data1.reset_index(drop=True)\n",
    "data2 = data2.drop(['id'], axis=1)\n",
    "data2 = data2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e4365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicates from the dataframe\n",
    "data1.duplicated(subset= ['article', 'highlights']).sum()\n",
    "data1 = data1.drop_duplicates(subset= ['article', 'highlights'])\n",
    "\n",
    "data2.duplicated(subset= ['article', 'highlights']).sum()\n",
    "data2 = data2.drop_duplicates(subset= ['article', 'highlights'])\n",
    "\n",
    "df.duplicated(subset= ['article', 'highlights']).sum()\n",
    "df = df.drop_duplicates(subset= ['article', 'highlights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b38a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in article column\n",
    "print(df['article'].isna().sum())\n",
    "df.dropna(axis=0,inplace=True)\n",
    "df = df.dropna(subset=['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27548493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values using SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "df['article'] = imp.fit_transform(df[['article']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07055d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resset the dataset\n",
    "df = df.reset_index(drop=True)\n",
    "data1 = data1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "547c82d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in OriginalTweet column\n",
    "print(df['article'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaf58277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################### TEST #1 ######################\n",
      "################### DECISION TREE ######################\n",
      "\n",
      "[[ 656   68  559   86  201]\n",
      " [  46  815  165   55  678]\n",
      " [ 375  166 1313  318  573]\n",
      " [  62   67  280 1434  262]\n",
      " [ 136  462  514  282 1666]]\n",
      "\\Accuracy score: 0.5235341222528694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\9941064513.UPS\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################### TEST #3 ######################\n",
      "################### KNN ######################\n",
      "\n",
      "[[ 656   68  559   86  201]\n",
      " [  46  815  165   55  678]\n",
      " [ 375  166 1313  318  573]\n",
      " [  62   67  280 1434  262]\n",
      " [ 136  462  514  282 1666]]\n",
      "\n",
      "Accuracy score: 0.2652371207402794\n",
      "\n",
      "################### TEST #5 ######################\n",
      "################### RANDOM FOREST ######################\n",
      "\n",
      "[[ 656   68  559   86  201]\n",
      " [  46  815  165   55  678]\n",
      " [ 375  166 1313  318  573]\n",
      " [  62   67  280 1434  262]\n",
      " [ 136  462  514  282 1666]]\n",
      "\n",
      "Accuracy score: 0.48331702108728536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\9941064513.UPS\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################### TEST #2 ######################\n",
      "################### LOGISTIC REGRESSION ######################\n",
      "\n",
      "[[ 656   68  559   86  201]\n",
      " [  46  815  165   55  678]\n",
      " [ 375  166 1313  318  573]\n",
      " [  62   67  280 1434  262]\n",
      " [ 136  462  514  282 1666]]\n",
      "\n",
      "Accuracy score: 0.6107304920366581\n",
      "\n",
      "################### TEST #4 ######################\n",
      "################### NAIVE BAYES ######################\n",
      "\n",
      "[[ 656   68  559   86  201]\n",
      " [  46  815  165   55  678]\n",
      " [ 375  166 1313  318  573]\n",
      " [  62   67  280 1434  262]\n",
      " [ 136  462  514  282 1666]]\n",
      "\n",
      "Accuracy score: 0.30589910134353593\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "corpus = []\n",
    "ps = PorterStemmer()\n",
    "for i in range(0, df.shape[0]):\n",
    "    if i in df.index:\n",
    "        review = re.sub('[^a-zA-Z]', ' ', df['article'][i])\n",
    "        review = review.lower()\n",
    "        review = review.split()\n",
    "        review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "        review = ' '.join(review)\n",
    "        corpus.append(review)\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "cv = CountVectorizer(max_features=5000) \n",
    "\n",
    "# Generate feature matrix and target variable\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = df.iloc[:, -1].values\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "\n",
    "# ---------------------------- DECISION TREE ------------------------------ #\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=0)\n",
    "\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(\"\\n################### TEST #1 ######################\")\n",
    "print(\"################### DECISION TREE ######################\\n\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "print(cm)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f\"\\Accuracy score: {accuracy_score(y_test,y_pred)}\")\n",
    "\n",
    "\n",
    "# --------------------------------- KNN ----------------------------------- #\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(\"\\n################### TEST #3 ######################\")\n",
    "print(\"################### KNN ######################\\n\")\n",
    "\n",
    "print(cm)\n",
    "\n",
    "print(f\"\\nAccuracy score: {accuracy_score(y_test,y_pred)}\")\n",
    "\n",
    "\n",
    "# ---------------------------- RANDOM FOREST ------------------------------ #\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=5)\n",
    "\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(\"\\n################### TEST #5 ######################\")\n",
    "print(\"################### RANDOM FOREST ######################\\n\")\n",
    "\n",
    "print(cm)\n",
    "\n",
    "print(f\"\\nAccuracy score: {accuracy_score(y_test,y_pred)}\")\n",
    "\n",
    "\n",
    "# ------------------------- LOGISTIC REGRESSION --------------------------- #\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(\"\\n################### TEST #2 ######################\")\n",
    "print(\"################### LOGISTIC REGRESSION ######################\\n\")\n",
    "\n",
    "print(cm)\n",
    "\n",
    "print(f\"\\nAccuracy score: {accuracy_score(y_test,y_pred)}\")\n",
    "\n",
    "\n",
    "# ----------------------------- NAIVE BAYES ------------------------------- #\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier = GaussianNB()\n",
    "\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(\"\\n################### TEST #4 ######################\")\n",
    "print(\"################### NAIVE BAYES ######################\\n\")\n",
    "\n",
    "print(cm)\n",
    "\n",
    "print(f\"\\nAccuracy score: {accuracy_score(y_test,y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761876a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
